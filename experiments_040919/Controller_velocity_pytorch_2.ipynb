{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: CUDA-fy, add \"resume\" functionality, punish no-ops less\n",
    "\n",
    "#### Credit:\n",
    "Largely adapted from Andrej Karpathy's pong playing agent and [this notebook](https://gist.github.com/ts1829/ebbe2cf946bf36951b724818c52e36b9#file-policy-gradient-with-cartpole-and-pytorch-medium-version-ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuda = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple Controller module that attempts to navigate to randomly generated target locations\n",
    " - Reward based on getting within 7 pixels of target location and having zero velocity\n",
    " - Use paddle velocity as input to the reinforcement learning algo as well\n",
    " \"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, D_in, h1=128):\n",
    "        super(Policy, self).__init__()\n",
    "        #self.state_space = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "        \n",
    "        self.l1 = nn.Linear(D_in, h1, bias=False)\n",
    "        self.l2 = nn.Linear(h1, self.action_space, bias=False)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Episode policy and reward history \n",
    "        self.policy_history = Variable(torch.Tensor()).to(device=device) \n",
    "        self.reward_episode = []\n",
    "        # Overall reward and loss history\n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "\n",
    "    def forward(self, x):    \n",
    "        model = torch.nn.Sequential(\n",
    "            self.l1,\n",
    "            #nn.Dropout(p=0.6),\n",
    "            nn.ReLU(),\n",
    "            self.l2,\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        #print(x)\n",
    "        #print(self.l1.weight)\n",
    "        #print(self.l1.bias)\n",
    "        #print()\n",
    "        return model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy2(nn.Module):\n",
    "    def __init__(self, D_in, h1=128, h2=64):\n",
    "        super(Policy2, self).__init__()\n",
    "        #self.state_space = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "        \n",
    "        self.l1 = nn.Linear(D_in, h1, bias=False)\n",
    "        self.l2 = nn.Linear(h1, h2, bias=False)\n",
    "        self.l3 = nn.Linear(h2, self.action_space, bias=False)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Episode policy and reward history \n",
    "        self.policy_history = Variable(torch.Tensor()).to(device=device) \n",
    "        self.reward_episode = []\n",
    "        # Overall reward and loss history\n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "\n",
    "    def forward(self, x):    \n",
    "        model = torch.nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.Dropout(p=0.6),\n",
    "            nn.ReLU(),\n",
    "            self.l2,\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "            self.l3,\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        #print(x)\n",
    "        #print(self.l1.weight)\n",
    "        #print(self.l1.bias)\n",
    "        #print()\n",
    "        return model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_policy():\n",
    "    R = 0\n",
    "    rewards = []\n",
    "    \n",
    "    # Discount future rewards back to the present using gamma\n",
    "    for r in policy.reward_episode[::-1]:\n",
    "        R = r + policy.gamma * R\n",
    "        rewards.insert(0,R)\n",
    "        \n",
    "    # Scale rewards\n",
    "    rewards = torch.FloatTensor(rewards).to(device=device)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = (torch.sum(torch.mul(policy.policy_history, Variable(rewards)).mul(-1), -1)).to(device=device)\n",
    "    \n",
    "    # Update network weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    #torch.nn.utils.clip_grad_norm_(policy.parameters(), 2)\n",
    "    optimizer.step()\n",
    "    \n",
    "    #Save and intialize episode history counters\n",
    "    policy.loss_history.append(loss.item())\n",
    "    policy.reward_history.append(np.sum(policy.reward_episode))\n",
    "    policy.policy_history = Variable(torch.Tensor())\n",
    "    policy.reward_episode= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    #Select an action (0 or 1) by running policy model and choosing based on the probabilities in state\n",
    "    state = torch.from_numpy(state).type(torch.FloatTensor).to(device=device)\n",
    "    #print(state.numpy())\n",
    "    state = policy(Variable(state))\n",
    "    c = Categorical(state)\n",
    "    #print(state.detach().numpy())\n",
    "    #print()\n",
    "    action = c.sample()\n",
    "    \n",
    "    # Add log probability of our chosen action to our history    \n",
    "    if policy.policy_history.dim() != 0:\n",
    "        #print(policy.policy_history.dim(), c.log_prob(action).dim())\n",
    "        policy.policy_history = torch.cat([policy.policy_history.to(device=device), \n",
    "                                           c.log_prob(action).view(1).to(device=device)]).to(device=device)\n",
    "    else:\n",
    "        policy.policy_history = (c.log_prob(action)).to(device=device)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    #I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    #I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    I = I[:-1,:,0]\n",
    "    return I.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_paddle_y(img, display_message=False):\n",
    "    paddle_2_x = 139 # Leftmost position of paddle 2\n",
    "    paddle_height = 15\n",
    "\n",
    "    paddle_1_color = 213\n",
    "    paddle_2_color = 92\n",
    "    ball_color = 236\n",
    "\n",
    "    ## In the beginning of the game, the paddle on the left and the ball are not yet present\n",
    "    not_all_present = np.where(img == paddle_2_color)[0].size == 0\n",
    "    if (not_all_present):\n",
    "        if display_message:\n",
    "            print(\"One or more of the objects is missing, returning an empty list of positions\")\n",
    "            print(\"(This happens at the first few steps of the game)\")\n",
    "        return -1\n",
    "\n",
    "    paddle_2_top = np.unique(np.where(img == paddle_2_color)[0])[0]\n",
    "    paddle_2_bot = paddle_2_top + paddle_height\n",
    "\n",
    "    return (paddle_2_top + paddle_2_bot) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward for this ep(0): 26.25\n",
      "This epsiode lasted 1392 steps\n",
      "Total reward for this ep(1): 21.20\n",
      "This epsiode lasted 1144 steps\n",
      "Total reward for this ep(2): 19.96\n",
      "This epsiode lasted 1017 steps\n",
      "Total reward for this ep(3): 7.94\n",
      "This epsiode lasted 1215 steps\n",
      "Total reward for this ep(4): 13.50\n",
      "This epsiode lasted 1161 steps\n",
      "Total reward for this ep(5): 17.33\n",
      "This epsiode lasted 1029 steps\n",
      "Total reward for this ep(6): 21.51\n",
      "This epsiode lasted 1364 steps\n",
      "Total reward for this ep(7): 17.55\n",
      "This epsiode lasted 1258 steps\n",
      "Total reward for this ep(8): 9.91\n",
      "This epsiode lasted 1018 steps\n",
      "Total reward for this ep(9): 29.94\n",
      "This epsiode lasted 1274 steps\n",
      "Total reward for this ep(10): 11.78\n",
      "This epsiode lasted 1082 steps\n",
      "Total reward for this ep(11): 33.82\n",
      "This epsiode lasted 1388 steps\n",
      "Total reward for this ep(12): 34.17\n",
      "This epsiode lasted 1102 steps\n",
      "Total reward for this ep(13): 13.56\n",
      "This epsiode lasted 1155 steps\n",
      "Total reward for this ep(14): 18.23\n",
      "This epsiode lasted 1190 steps\n",
      "Total reward for this ep(15): 26.38\n",
      "This epsiode lasted 1128 steps\n",
      "Total reward for this ep(16): 37.54\n",
      "This epsiode lasted 1518 steps\n",
      "Total reward for this ep(17): 15.00\n",
      "This epsiode lasted 1011 steps\n",
      "Total reward for this ep(18): 28.68\n",
      "This epsiode lasted 1149 steps\n",
      "Total reward for this ep(19): 17.45\n",
      "This epsiode lasted 1017 steps\n",
      "Total reward for this ep(20): 24.92\n",
      "This epsiode lasted 1023 steps\n",
      "Total reward for this ep(21): 42.46\n",
      "This epsiode lasted 1277 steps\n",
      "Total reward for this ep(22): 21.49\n",
      "This epsiode lasted 1115 steps\n",
      "Total reward for this ep(23): 27.59\n",
      "This epsiode lasted 1258 steps\n",
      "Total reward for this ep(24): 16.03\n",
      "This epsiode lasted 1159 steps\n",
      "Total reward for this ep(25): 33.17\n",
      "This epsiode lasted 1202 steps\n",
      "Total reward for this ep(26): 36.77\n",
      "This epsiode lasted 1093 steps\n",
      "Total reward for this ep(27): 22.31\n",
      "This epsiode lasted 1033 steps\n",
      "Total reward for this ep(28): 12.43\n",
      "This epsiode lasted 1017 steps\n",
      "Total reward for this ep(29): 6.99\n",
      "This epsiode lasted 1310 steps\n",
      "Total reward for this ep(30): 32.47\n",
      "This epsiode lasted 1021 steps\n",
      "Total reward for this ep(31): 31.45\n",
      "This epsiode lasted 1123 steps\n",
      "Total reward for this ep(32): 29.93\n",
      "This epsiode lasted 1024 steps\n",
      "Total reward for this ep(33): 36.87\n",
      "This epsiode lasted 1083 steps\n",
      "Total reward for this ep(34): 17.52\n",
      "This epsiode lasted 1261 steps\n",
      "Total reward for this ep(35): 19.89\n",
      "This epsiode lasted 1024 steps\n",
      "Total reward for this ep(36): 34.20\n",
      "This epsiode lasted 1099 steps\n",
      "Total reward for this ep(37): 20.01\n",
      "This epsiode lasted 1012 steps\n",
      "Total reward for this ep(38): 28.84\n",
      "This epsiode lasted 1133 steps\n",
      "Total reward for this ep(39): 29.21\n",
      "This epsiode lasted 1096 steps\n",
      "Total reward for this ep(40): 24.98\n",
      "This epsiode lasted 1017 steps\n",
      "Total reward for this ep(41): 27.52\n",
      "This epsiode lasted 1014 steps\n",
      "Total reward for this ep(42): 29.19\n",
      "This epsiode lasted 1098 steps\n",
      "Total reward for this ep(43): 42.33\n",
      "This epsiode lasted 1039 steps\n",
      "Total reward for this ep(44): 38.93\n",
      "This epsiode lasted 1379 steps\n",
      "Total reward for this ep(45): 25.48\n",
      "This epsiode lasted 1218 steps\n",
      "Total reward for this ep(46): 17.48\n",
      "This epsiode lasted 1014 steps\n",
      "Total reward for this ep(47): 30.04\n",
      "This epsiode lasted 1264 steps\n",
      "Total reward for this ep(48): 12.37\n",
      "This epsiode lasted 1023 steps\n",
      "Total reward for this ep(49): 24.83\n",
      "This epsiode lasted 1534 steps\n",
      "Total reward for this ep(50): 37.17\n",
      "This epsiode lasted 1053 steps\n",
      "Total reward for this ep(51): 53.41\n",
      "This epsiode lasted 1186 steps\n",
      "Total reward for this ep(52): 20.01\n",
      "This epsiode lasted 1012 steps\n",
      "Total reward for this ep(53): 55.67\n",
      "This epsiode lasted 1462 steps\n",
      "Total reward for this ep(54): 39.71\n",
      "This epsiode lasted 1050 steps\n",
      "Total reward for this ep(55): 35.62\n",
      "This epsiode lasted 1208 steps\n",
      "Total reward for this ep(56): 27.94\n",
      "This epsiode lasted 1223 steps\n",
      "Total reward for this ep(57): 24.65\n",
      "This epsiode lasted 1301 steps\n",
      "Total reward for this ep(58): 15.04\n",
      "This epsiode lasted 1007 steps\n",
      "Total reward for this ep(59): 50.46\n",
      "This epsiode lasted 1230 steps\n",
      "Total reward for this ep(60): 20.74\n",
      "This epsiode lasted 1190 steps\n",
      "Total reward for this ep(61): 19.59\n",
      "This epsiode lasted 1054 steps\n",
      "Total reward for this ep(62): 44.06\n",
      "This epsiode lasted 1117 steps\n",
      "Total reward for this ep(63): 25.05\n",
      "This epsiode lasted 1010 steps\n",
      "Total reward for this ep(64): 33.10\n",
      "This epsiode lasted 1209 steps\n",
      "Total reward for this ep(65): 26.41\n",
      "This epsiode lasted 1125 steps\n",
      "Total reward for this ep(66): 33.86\n",
      "This epsiode lasted 1133 steps\n",
      "Total reward for this ep(67): 29.86\n",
      "This epsiode lasted 1031 steps\n",
      "Total reward for this ep(68): 40.57\n",
      "This epsiode lasted 1215 steps\n",
      "Total reward for this ep(69): 47.98\n",
      "This epsiode lasted 1227 steps\n",
      "Total reward for this ep(70): 22.43\n",
      "This epsiode lasted 1021 steps\n",
      "Total reward for this ep(71): 42.31\n",
      "This epsiode lasted 1041 steps\n",
      "Total reward for this ep(72): 29.06\n",
      "This epsiode lasted 1111 steps\n",
      "Total reward for this ep(73): 34.41\n",
      "This epsiode lasted 1078 steps\n",
      "Total reward for this ep(74): 34.26\n",
      "This epsiode lasted 1093 steps\n",
      "Total reward for this ep(75): 41.58\n",
      "This epsiode lasted 1114 steps\n",
      "Total reward for this ep(76): 16.51\n",
      "This epsiode lasted 1111 steps\n",
      "Total reward for this ep(77): 31.36\n",
      "This epsiode lasted 1132 steps\n",
      "Total reward for this ep(78): 28.12\n",
      "This epsiode lasted 1205 steps\n",
      "Total reward for this ep(79): 74.05\n",
      "This epsiode lasted 1381 steps\n",
      "Total reward for this ep(80): 27.14\n",
      "This epsiode lasted 1052 steps\n",
      "Total reward for this ep(81): 34.81\n",
      "This epsiode lasted 1038 steps\n",
      "Total reward for this ep(82): 45.29\n",
      "This epsiode lasted 1496 steps\n",
      "Total reward for this ep(83): 22.82\n",
      "This epsiode lasted 1233 steps\n",
      "Total reward for this ep(84): 39.29\n",
      "This epsiode lasted 1092 steps\n",
      "Total reward for this ep(85): 62.56\n",
      "This epsiode lasted 1024 steps\n",
      "Total reward for this ep(86): 35.56\n",
      "This epsiode lasted 1214 steps\n",
      "Total reward for this ep(87): 29.75\n",
      "This epsiode lasted 1042 steps\n",
      "Total reward for this ep(88): 31.70\n",
      "This epsiode lasted 1098 steps\n",
      "Total reward for this ep(89): 40.93\n",
      "This epsiode lasted 1179 steps\n",
      "Total reward for this ep(90): 27.41\n",
      "This epsiode lasted 1025 steps\n",
      "Total reward for this ep(91): 33.65\n",
      "This epsiode lasted 1154 steps\n",
      "Total reward for this ep(92): 27.36\n",
      "This epsiode lasted 1030 steps\n",
      "Total reward for this ep(93): 18.57\n",
      "This epsiode lasted 1156 steps\n",
      "Total reward for this ep(94): 23.80\n",
      "This epsiode lasted 1135 steps\n",
      "Total reward for this ep(95): 34.97\n",
      "This epsiode lasted 1273 steps\n",
      "Total reward for this ep(96): 40.70\n",
      "This epsiode lasted 1202 steps\n",
      "Total reward for this ep(97): 58.75\n",
      "This epsiode lasted 1154 steps\n",
      "Total reward for this ep(98): 28.03\n",
      "This epsiode lasted 1214 steps\n",
      "Total reward for this ep(99): 33.14\n",
      "This epsiode lasted 1205 steps\n",
      "Total reward for this ep(100): 32.17\n",
      "This epsiode lasted 1051 steps\n",
      "Total reward for this ep(101): 31.04\n",
      "This epsiode lasted 1164 steps\n",
      "Total reward for this ep(102): 36.17\n",
      "This epsiode lasted 1404 steps\n",
      "Total reward for this ep(103): 27.19\n",
      "This epsiode lasted 1047 steps\n",
      "Total reward for this ep(104): 33.45\n",
      "This epsiode lasted 1174 steps\n",
      "Total reward for this ep(105): 53.79\n",
      "This epsiode lasted 1148 steps\n",
      "Total reward for this ep(106): 40.00\n",
      "This epsiode lasted 1021 steps\n",
      "Total reward for this ep(107): 52.58\n",
      "This epsiode lasted 1269 steps\n",
      "Total reward for this ep(108): 35.64\n",
      "This epsiode lasted 1206 steps\n",
      "Total reward for this ep(109): 39.64\n",
      "This epsiode lasted 1308 steps\n",
      "Total reward for this ep(110): 37.83\n",
      "This epsiode lasted 1238 steps\n",
      "Total reward for this ep(111): 39.78\n",
      "This epsiode lasted 1043 steps\n",
      "Total reward for this ep(112): 42.63\n",
      "This epsiode lasted 1009 steps\n",
      "Total reward for this ep(113): 21.94\n",
      "This epsiode lasted 1070 steps\n",
      "Total reward for this ep(114): 56.53\n",
      "This epsiode lasted 1376 steps\n",
      "Total reward for this ep(115): 34.57\n",
      "This epsiode lasted 1062 steps\n",
      "Total reward for this ep(116): 63.26\n",
      "This epsiode lasted 1205 steps\n",
      "Total reward for this ep(117): 31.58\n",
      "This epsiode lasted 1361 steps\n",
      "Total reward for this ep(118): 31.39\n",
      "This epsiode lasted 1129 steps\n",
      "Total reward for this ep(119): 33.72\n",
      "This epsiode lasted 1147 steps\n",
      "Total reward for this ep(120): 39.37\n",
      "This epsiode lasted 1084 steps\n",
      "Total reward for this ep(121): 37.09\n",
      "This epsiode lasted 1061 steps\n",
      "Total reward for this ep(122): 24.59\n",
      "This epsiode lasted 1056 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward for this ep(123): 56.02\n",
      "This epsiode lasted 1176 steps\n",
      "Total reward for this ep(124): 38.81\n",
      "This epsiode lasted 1140 steps\n",
      "Total reward for this ep(125): 49.22\n",
      "This epsiode lasted 1103 steps\n",
      "Total reward for this ep(126): 46.07\n",
      "This epsiode lasted 1167 steps\n",
      "Total reward for this ep(127): 47.53\n",
      "This epsiode lasted 1272 steps\n",
      "Total reward for this ep(128): 67.65\n",
      "This epsiode lasted 1017 steps\n",
      "Total reward for this ep(129): 56.17\n",
      "This epsiode lasted 1161 steps\n",
      "Total reward for this ep(130): 47.42\n",
      "This epsiode lasted 1032 steps\n",
      "Total reward for this ep(131): 27.42\n",
      "This epsiode lasted 1024 steps\n",
      "Total reward for this ep(132): 32.05\n",
      "This epsiode lasted 1063 steps\n",
      "Total reward for this ep(133): 56.72\n",
      "This epsiode lasted 1106 steps\n",
      "Total reward for this ep(134): 42.18\n",
      "This epsiode lasted 1054 steps\n",
      "Total reward for this ep(135): 44.54\n",
      "This epsiode lasted 1069 steps\n",
      "Total reward for this ep(136): 46.73\n",
      "This epsiode lasted 1101 steps\n",
      "Total reward for this ep(137): 47.70\n",
      "This epsiode lasted 1004 steps\n",
      "Total reward for this ep(138): 54.00\n",
      "This epsiode lasted 1127 steps\n",
      "Total reward for this ep(139): 37.98\n",
      "This epsiode lasted 1223 steps\n",
      "Total reward for this ep(140): 34.92\n",
      "This epsiode lasted 1027 steps\n",
      "Total reward for this ep(141): 59.33\n",
      "This epsiode lasted 1347 steps\n",
      "Total reward for this ep(142): 41.82\n",
      "This epsiode lasted 1090 steps\n",
      "Total reward for this ep(143): 57.96\n",
      "This epsiode lasted 1233 steps\n",
      "Total reward for this ep(144): 69.98\n",
      "This epsiode lasted 1286 steps\n",
      "Total reward for this ep(145): 48.41\n",
      "This epsiode lasted 1184 steps\n",
      "Total reward for this ep(146): 34.17\n",
      "This epsiode lasted 1102 steps\n",
      "Total reward for this ep(147): 47.11\n",
      "This epsiode lasted 1314 steps\n",
      "Total reward for this ep(148): 47.04\n",
      "This epsiode lasted 1070 steps\n",
      "Total reward for this ep(149): 45.41\n",
      "This epsiode lasted 1233 steps\n",
      "Total reward for this ep(150): 44.99\n",
      "This epsiode lasted 1024 steps\n",
      "Total reward for this ep(151): 44.59\n",
      "This epsiode lasted 1064 steps\n",
      "Total reward for this ep(152): 48.72\n",
      "This epsiode lasted 1153 steps\n",
      "Total reward for this ep(153): 58.05\n",
      "This epsiode lasted 1224 steps\n",
      "Total reward for this ep(154): 42.49\n",
      "This epsiode lasted 1023 steps\n",
      "Total reward for this ep(155): 50.16\n",
      "This epsiode lasted 1260 steps\n",
      "Total reward for this ep(156): 68.89\n",
      "This epsiode lasted 1144 steps\n",
      "Total reward for this ep(157): 40.08\n",
      "This epsiode lasted 1013 steps\n",
      "Total reward for this ep(158): 63.08\n",
      "This epsiode lasted 1223 steps\n",
      "Total reward for this ep(159): 58.11\n",
      "This epsiode lasted 1218 steps\n",
      "Total reward for this ep(160): 47.72\n",
      "This epsiode lasted 1002 steps\n",
      "Total reward for this ep(161): 66.82\n",
      "This epsiode lasted 1100 steps\n",
      "Total reward for this ep(162): 75.75\n",
      "This epsiode lasted 1211 steps\n",
      "Total reward for this ep(163): 49.99\n",
      "This epsiode lasted 1026 steps\n",
      "Total reward for this ep(164): 59.90\n",
      "This epsiode lasted 1290 steps\n",
      "Total reward for this ep(165): 69.08\n",
      "This epsiode lasted 1125 steps\n",
      "Total reward for this ep(166): 51.20\n",
      "This epsiode lasted 1407 steps\n",
      "Total reward for this ep(167): 50.07\n",
      "This epsiode lasted 1018 steps\n",
      "Total reward for this ep(168): 62.56\n",
      "This epsiode lasted 1024 steps\n",
      "Total reward for this ep(169): 42.12\n",
      "This epsiode lasted 1060 steps\n",
      "Total reward for this ep(170): 44.14\n",
      "This epsiode lasted 1109 steps\n",
      "Total reward for this ep(171): 44.27\n",
      "This epsiode lasted 1096 steps\n",
      "Total reward for this ep(172): 59.09\n",
      "This epsiode lasted 1120 steps\n",
      "Total reward for this ep(173): 60.76\n",
      "This epsiode lasted 1204 steps\n",
      "Total reward for this ep(174): 69.17\n",
      "This epsiode lasted 1367 steps\n",
      "Total reward for this ep(175): 63.76\n",
      "This epsiode lasted 1155 steps\n",
      "Total reward for this ep(176): 65.12\n",
      "This epsiode lasted 1019 steps\n",
      "Total reward for this ep(177): 57.24\n",
      "This epsiode lasted 1305 steps\n",
      "Total reward for this ep(178): 57.65\n",
      "This epsiode lasted 1013 steps\n",
      "Total reward for this ep(179): 47.21\n",
      "This epsiode lasted 1053 steps\n",
      "Total reward for this ep(180): 64.69\n",
      "This epsiode lasted 1062 steps\n",
      "Total reward for this ep(181): 75.56\n",
      "This epsiode lasted 1230 steps\n",
      "Total reward for this ep(182): 69.69\n",
      "This epsiode lasted 1064 steps\n",
      "Total reward for this ep(183): 71.90\n",
      "This epsiode lasted 1094 steps\n",
      "Total reward for this ep(184): 71.15\n",
      "This epsiode lasted 1169 steps\n",
      "Total reward for this ep(185): 66.15\n",
      "This epsiode lasted 1167 steps\n",
      "Total reward for this ep(186): 67.65\n",
      "This epsiode lasted 1268 steps\n",
      "Total reward for this ep(187): 48.95\n",
      "This epsiode lasted 1130 steps\n",
      "Total reward for this ep(188): 80.85\n",
      "This epsiode lasted 1203 steps\n",
      "Total reward for this ep(189): 39.79\n",
      "This epsiode lasted 1042 steps\n",
      "Total reward for this ep(190): 53.68\n",
      "This epsiode lasted 1159 steps\n",
      "Total reward for this ep(191): 57.02\n",
      "This epsiode lasted 1076 steps\n",
      "Total reward for this ep(192): 69.67\n",
      "This epsiode lasted 1066 steps\n",
      "Total reward for this ep(193): 47.56\n",
      "This epsiode lasted 1018 steps\n",
      "Total reward for this ep(194): 52.51\n",
      "This epsiode lasted 1025 steps\n",
      "Total reward for this ep(195): 50.04\n",
      "This epsiode lasted 1021 steps\n",
      "Total reward for this ep(196): 83.20\n",
      "This epsiode lasted 1219 steps\n",
      "Total reward for this ep(197): 54.78\n",
      "This epsiode lasted 1300 steps\n",
      "Total reward for this ep(198): 59.86\n",
      "This epsiode lasted 1043 steps\n",
      "Total reward for this ep(199): 84.46\n",
      "This epsiode lasted 1093 steps\n",
      "Total reward for this ep(200): 54.90\n",
      "This epsiode lasted 1037 steps\n",
      "Total reward for this ep(201): 60.01\n",
      "This epsiode lasted 1028 steps\n",
      "Total reward for this ep(202): 59.74\n",
      "This epsiode lasted 1055 steps\n",
      "Total reward for this ep(203): 70.43\n",
      "This epsiode lasted 1241 steps\n",
      "Total reward for this ep(204): 59.34\n",
      "This epsiode lasted 1095 steps\n",
      "Total reward for this ep(205): 59.35\n",
      "This epsiode lasted 1094 steps\n",
      "Total reward for this ep(206): 37.37\n",
      "This epsiode lasted 1033 steps\n",
      "Total reward for this ep(207): 77.59\n",
      "This epsiode lasted 1027 steps\n",
      "Total reward for this ep(208): 59.74\n",
      "This epsiode lasted 1055 steps\n",
      "Total reward for this ep(209): 67.27\n",
      "This epsiode lasted 1055 steps\n",
      "Total reward for this ep(210): 53.95\n",
      "This epsiode lasted 1132 steps\n",
      "Total reward for this ep(211): 58.02\n",
      "This epsiode lasted 1227 steps\n",
      "Total reward for this ep(212): 72.21\n",
      "This epsiode lasted 1063 steps\n",
      "Total reward for this ep(213): 62.69\n",
      "This epsiode lasted 1011 steps\n",
      "Total reward for this ep(214): 57.66\n",
      "This epsiode lasted 1012 steps\n",
      "Total reward for this ep(215): 66.20\n",
      "This epsiode lasted 1162 steps\n",
      "Total reward for this ep(216): 27.78\n",
      "This epsiode lasted 1239 steps\n",
      "Total reward for this ep(217): 51.44\n",
      "This epsiode lasted 1132 steps\n",
      "Total reward for this ep(218): 44.94\n",
      "This epsiode lasted 1029 steps\n",
      "Total reward for this ep(219): 75.08\n",
      "This epsiode lasted 1027 steps\n",
      "Total reward for this ep(220): 67.12\n",
      "This epsiode lasted 1070 steps\n",
      "Total reward for this ep(221): 84.50\n",
      "This epsiode lasted 1089 steps\n",
      "Total reward for this ep(222): 55.08\n",
      "This epsiode lasted 1019 steps\n",
      "Total reward for this ep(223): 75.36\n",
      "This epsiode lasted 1250 steps\n",
      "Total reward for this ep(224): 87.91\n",
      "This epsiode lasted 1250 steps\n",
      "Total reward for this ep(225): 65.15\n",
      "This epsiode lasted 1016 steps\n",
      "Total reward for this ep(226): 55.16\n",
      "This epsiode lasted 1011 steps\n",
      "Total reward for this ep(227): 67.76\n",
      "This epsiode lasted 1006 steps\n",
      "Total reward for this ep(228): 70.25\n",
      "This epsiode lasted 1259 steps\n",
      "Total reward for this ep(229): 70.14\n",
      "This epsiode lasted 1019 steps\n",
      "Total reward for this ep(230): 70.12\n",
      "This epsiode lasted 1021 steps\n",
      "Total reward for this ep(231): 56.86\n",
      "This epsiode lasted 1092 steps\n",
      "Total reward for this ep(232): 81.93\n",
      "This epsiode lasted 1095 steps\n",
      "Total reward for this ep(233): 75.23\n",
      "This epsiode lasted 1012 steps\n",
      "Total reward for this ep(234): 69.76\n",
      "This epsiode lasted 1057 steps\n",
      "Total reward for this ep(235): 92.62\n",
      "This epsiode lasted 1030 steps\n",
      "Total reward for this ep(236): 47.57\n",
      "This epsiode lasted 1017 steps\n",
      "Total reward for this ep(237): 38.44\n",
      "This epsiode lasted 1177 steps\n",
      "Total reward for this ep(238): 66.98\n",
      "This epsiode lasted 1084 steps\n",
      "Total reward for this ep(239): 49.68\n",
      "This epsiode lasted 1057 steps\n",
      "Total reward for this ep(240): 72.53\n",
      "This epsiode lasted 1031 steps\n",
      "Total reward for this ep(241): 57.47\n",
      "This epsiode lasted 1031 steps\n",
      "Total reward for this ep(242): 73.11\n",
      "This epsiode lasted 1224 steps\n",
      "Total reward for this ep(243): 52.53\n",
      "This epsiode lasted 1023 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward for this ep(244): 69.79\n",
      "This epsiode lasted 1054 steps\n",
      "Total reward for this ep(245): 54.69\n",
      "This epsiode lasted 1058 steps\n",
      "Total reward for this ep(246): 69.25\n",
      "This epsiode lasted 1359 steps\n",
      "Total reward for this ep(247): 70.25\n",
      "This epsiode lasted 1008 steps\n",
      "Total reward for this ep(248): 67.51\n",
      "This epsiode lasted 1031 steps\n",
      "Total reward for this ep(249): 62.67\n",
      "This epsiode lasted 1013 steps\n",
      "Total reward for this ep(250): 89.11\n",
      "This epsiode lasted 1130 steps\n",
      "Total reward for this ep(251): 58.96\n",
      "This epsiode lasted 1133 steps\n",
      "Total reward for this ep(252): 69.50\n",
      "This epsiode lasted 1083 steps\n",
      "Total reward for this ep(253): 90.30\n",
      "This epsiode lasted 1262 steps\n",
      "Total reward for this ep(254): 77.68\n",
      "This epsiode lasted 1018 steps\n",
      "Total reward for this ep(255): 89.78\n",
      "This epsiode lasted 1314 steps\n",
      "Total reward for this ep(256): 86.82\n",
      "This epsiode lasted 1108 steps\n",
      "Total reward for this ep(257): 81.76\n",
      "This epsiode lasted 1112 steps\n",
      "Total reward for this ep(258): 62.51\n",
      "This epsiode lasted 1029 steps\n",
      "Total reward for this ep(259): 91.62\n",
      "This epsiode lasted 1130 steps\n",
      "Total reward for this ep(260): 77.67\n",
      "This epsiode lasted 1019 steps\n",
      "Total reward for this ep(261): 74.33\n",
      "This epsiode lasted 1102 steps\n",
      "Total reward for this ep(262): 90.21\n",
      "This epsiode lasted 1020 steps\n",
      "Total reward for this ep(263): 73.69\n",
      "This epsiode lasted 1166 steps\n",
      "Total reward for this ep(264): 54.85\n",
      "This epsiode lasted 1042 steps\n",
      "Total reward for this ep(265): 47.16\n",
      "This epsiode lasted 1058 steps\n",
      "Total reward for this ep(266): 59.03\n",
      "This epsiode lasted 1126 steps\n",
      "Total reward for this ep(267): 77.72\n",
      "This epsiode lasted 1014 steps\n",
      "Total reward for this ep(268): 99.43\n",
      "This epsiode lasted 1102 steps\n",
      "Total reward for this ep(269): 70.09\n",
      "This epsiode lasted 1024 steps\n",
      "Total reward for this ep(270): 92.21\n",
      "This epsiode lasted 1071 steps\n",
      "Total reward for this ep(271): 92.63\n",
      "This epsiode lasted 1029 steps\n",
      "Total reward for this ep(272): 87.38\n",
      "This epsiode lasted 1052 steps\n",
      "Total reward for this ep(273): 121.41\n",
      "This epsiode lasted 1163 steps\n",
      "Total reward for this ep(274): 111.53\n",
      "This epsiode lasted 1147 steps\n",
      "Total reward for this ep(275): 75.18\n",
      "This epsiode lasted 1017 steps\n",
      "Total reward for this ep(276): 52.49\n",
      "This epsiode lasted 1027 steps\n",
      "Total reward for this ep(277): 74.76\n",
      "This epsiode lasted 1059 steps\n",
      "Total reward for this ep(278): 78.78\n",
      "This epsiode lasted 1159 steps\n",
      "Total reward for this ep(279): 74.07\n",
      "This epsiode lasted 1128 steps\n",
      "Total reward for this ep(280): 67.62\n",
      "This epsiode lasted 1020 steps\n",
      "Total reward for this ep(281): 106.97\n",
      "This epsiode lasted 1352 steps\n",
      "Total reward for this ep(282): 90.26\n",
      "This epsiode lasted 1015 steps\n",
      "Total reward for this ep(283): 80.41\n",
      "This epsiode lasted 996 steps\n",
      "Total reward for this ep(284): 75.03\n",
      "This epsiode lasted 1032 steps\n",
      "Total reward for this ep(285): 72.71\n",
      "This epsiode lasted 1013 steps\n",
      "Total reward for this ep(286): 77.34\n",
      "This epsiode lasted 1052 steps\n",
      "Total reward for this ep(287): 67.57\n",
      "This epsiode lasted 1025 steps\n",
      "Total reward for this ep(288): 90.29\n",
      "This epsiode lasted 1012 steps\n",
      "Total reward for this ep(289): 82.27\n",
      "This epsiode lasted 1061 steps\n",
      "Total reward for this ep(290): 53.63\n",
      "This epsiode lasted 1164 steps\n",
      "Total reward for this ep(291): 70.25\n",
      "This epsiode lasted 1008 steps\n",
      "Total reward for this ep(292): 72.63\n",
      "This epsiode lasted 1021 steps\n",
      "Total reward for this ep(293): 101.35\n",
      "This epsiode lasted 1161 steps\n",
      "Total reward for this ep(294): 83.40\n",
      "This epsiode lasted 1199 steps\n",
      "Total reward for this ep(295): 91.14\n",
      "This epsiode lasted 1178 steps\n",
      "Total reward for this ep(296): 97.65\n",
      "This epsiode lasted 1029 steps\n",
      "Total reward for this ep(297): 106.18\n",
      "This epsiode lasted 1431 steps\n",
      "Total reward for this ep(298): 72.52\n",
      "This epsiode lasted 1032 steps\n",
      "Total reward for this ep(299): 95.21\n",
      "This epsiode lasted 1022 steps\n",
      "Total reward for this ep(300): 104.43\n",
      "This epsiode lasted 1104 steps\n",
      "Total reward for this ep(301): 65.02\n",
      "This epsiode lasted 1029 steps\n",
      "Total reward for this ep(302): 70.05\n",
      "This epsiode lasted 1028 steps\n",
      "Total reward for this ep(303): 117.71\n",
      "This epsiode lasted 1031 steps\n",
      "Total reward for this ep(304): 94.00\n",
      "This epsiode lasted 1143 steps\n",
      "Total reward for this ep(305): 110.95\n",
      "This epsiode lasted 1205 steps\n",
      "Total reward for this ep(306): 64.98\n",
      "This epsiode lasted 1033 steps\n",
      "Total reward for this ep(307): 97.38\n",
      "This epsiode lasted 1056 steps\n",
      "Total reward for this ep(308): 77.20\n",
      "This epsiode lasted 1066 steps\n",
      "Total reward for this ep(309): 75.18\n",
      "This epsiode lasted 1017 steps\n",
      "Total reward for this ep(310): 80.15\n",
      "This epsiode lasted 1022 steps\n",
      "Total reward for this ep(311): 91.95\n",
      "This epsiode lasted 1097 steps\n",
      "Total reward for this ep(312): 82.48\n",
      "This epsiode lasted 1040 steps\n",
      "Total reward for this ep(313): 90.29\n",
      "This epsiode lasted 1012 steps\n",
      "Total reward for this ep(314): 65.07\n",
      "This epsiode lasted 1024 steps\n",
      "Total reward for this ep(315): 94.69\n",
      "This epsiode lasted 1325 steps\n",
      "Total reward for this ep(316): 110.39\n",
      "This epsiode lasted 1010 steps\n",
      "Total reward for this ep(317): 104.17\n",
      "This epsiode lasted 1381 steps\n",
      "Total reward for this ep(318): 114.43\n",
      "This epsiode lasted 1108 steps\n",
      "Total reward for this ep(319): 80.26\n",
      "This epsiode lasted 1011 steps\n",
      "Total reward for this ep(320): 87.52\n",
      "This epsiode lasted 1038 steps\n",
      "Total reward for this ep(321): 80.10\n",
      "This epsiode lasted 1027 steps\n",
      "Total reward for this ep(322): 82.67\n",
      "This epsiode lasted 1021 steps\n",
      "Total reward for this ep(323): 117.87\n",
      "This epsiode lasted 1015 steps\n",
      "Total reward for this ep(324): 103.45\n",
      "This epsiode lasted 1453 steps\n",
      "Total reward for this ep(325): 51.90\n",
      "This epsiode lasted 1086 steps\n",
      "Total reward for this ep(326): 82.55\n",
      "This epsiode lasted 1033 steps\n",
      "Total reward for this ep(327): 87.78\n",
      "This epsiode lasted 1012 steps\n",
      "Total reward for this ep(328): 101.54\n",
      "This epsiode lasted 1142 steps\n",
      "Total reward for this ep(329): 100.30\n",
      "This epsiode lasted 1015 steps\n",
      "Total reward for this ep(330): 102.73\n",
      "This epsiode lasted 1023 steps\n",
      "Total reward for this ep(331): 74.76\n",
      "This epsiode lasted 1059 steps\n",
      "Total reward for this ep(332): 70.26\n",
      "This epsiode lasted 1007 steps\n",
      "Total reward for this ep(333): 102.27\n",
      "This epsiode lasted 1069 steps\n",
      "Total reward for this ep(334): 126.77\n",
      "This epsiode lasted 1129 steps\n",
      "Total reward for this ep(335): 81.94\n",
      "This epsiode lasted 1094 steps\n",
      "Total reward for this ep(336): 97.33\n",
      "This epsiode lasted 1061 steps\n",
      "Total reward for this ep(337): 87.72\n",
      "This epsiode lasted 1018 steps\n",
      "Total reward for this ep(338): 76.50\n",
      "This epsiode lasted 1136 steps\n",
      "Total reward for this ep(339): 77.64\n",
      "This epsiode lasted 1022 steps\n",
      "Total reward for this ep(340): 94.39\n",
      "This epsiode lasted 1104 steps\n",
      "Total reward for this ep(341): 104.68\n",
      "This epsiode lasted 1079 steps\n",
      "Total reward for this ep(342): 87.77\n",
      "This epsiode lasted 1013 steps\n",
      "Total reward for this ep(343): 52.50\n",
      "This epsiode lasted 1026 steps\n",
      "Total reward for this ep(344): 96.83\n",
      "This epsiode lasted 1111 steps\n",
      "Total reward for this ep(345): 60.18\n",
      "This epsiode lasted 1011 steps\n",
      "Total reward for this ep(346): 105.25\n",
      "This epsiode lasted 1022 steps\n",
      "Total reward for this ep(347): 100.29\n",
      "This epsiode lasted 1016 steps\n",
      "Total reward for this ep(348): 100.38\n",
      "This epsiode lasted 1007 steps\n",
      "Total reward for this ep(349): 79.69\n",
      "This epsiode lasted 1068 steps\n",
      "Total reward for this ep(350): 105.20\n",
      "This epsiode lasted 1027 steps\n",
      "Total reward for this ep(351): 75.11\n",
      "This epsiode lasted 1024 steps\n",
      "Total reward for this ep(352): 92.60\n",
      "This epsiode lasted 1032 steps\n",
      "Total reward for this ep(353): 70.07\n",
      "This epsiode lasted 1026 steps\n",
      "Total reward for this ep(354): 87.73\n",
      "This epsiode lasted 1017 steps\n",
      "Total reward for this ep(355): 77.69\n",
      "This epsiode lasted 1017 steps\n",
      "Total reward for this ep(356): 102.56\n",
      "This epsiode lasted 1291 steps\n",
      "Total reward for this ep(357): 94.90\n",
      "This epsiode lasted 1053 steps\n",
      "Total reward for this ep(358): 158.76\n",
      "This epsiode lasted 1444 steps\n",
      "Total reward for this ep(359): 102.37\n",
      "This epsiode lasted 1059 steps\n",
      "Total reward for this ep(360): 115.99\n",
      "This epsiode lasted 1203 steps\n",
      "Total reward for this ep(361): 82.66\n",
      "This epsiode lasted 1022 steps\n",
      "Total reward for this ep(362): 105.50\n",
      "This epsiode lasted 1248 steps\n",
      "Total reward for this ep(363): 75.03\n",
      "This epsiode lasted 1032 steps\n",
      "Total reward for this ep(364): 113.88\n",
      "This epsiode lasted 1163 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward for this ep(365): 107.84\n",
      "This epsiode lasted 1014 steps\n",
      "Total reward for this ep(366): 80.32\n",
      "This epsiode lasted 1005 steps\n",
      "Total reward for this ep(367): 110.27\n",
      "This epsiode lasted 1022 steps\n",
      "Total reward for this ep(368): 133.08\n",
      "This epsiode lasted 1251 steps\n",
      "Total reward for this ep(369): 75.10\n",
      "This epsiode lasted 1025 steps\n",
      "Total reward for this ep(370): 105.28\n",
      "This epsiode lasted 1270 steps\n",
      "Total reward for this ep(371): 95.14\n",
      "This epsiode lasted 1029 steps\n",
      "Total reward for this ep(372): 72.64\n",
      "This epsiode lasted 1020 steps\n",
      "Total reward for this ep(373): 100.20\n",
      "This epsiode lasted 1025 steps\n",
      "Total reward for this ep(374): 85.32\n",
      "This epsiode lasted 1007 steps\n",
      "Total reward for this ep(375): 122.67\n",
      "This epsiode lasted 1037 steps\n",
      "Total reward for this ep(376): 100.21\n",
      "This epsiode lasted 1024 steps\n",
      "Total reward for this ep(377): 85.20\n",
      "This epsiode lasted 1019 steps\n",
      "Total reward for this ep(378): 95.26\n",
      "This epsiode lasted 1017 steps\n",
      "Total reward for this ep(379): 116.48\n",
      "This epsiode lasted 1154 steps\n",
      "Total reward for this ep(380): 95.40\n",
      "This epsiode lasted 1003 steps\n",
      "Total reward for this ep(381): 122.84\n",
      "This epsiode lasted 1020 steps\n",
      "Total reward for this ep(382): 60.19\n",
      "This epsiode lasted 1010 steps\n",
      "Total reward for this ep(383): 92.74\n",
      "This epsiode lasted 1018 steps\n",
      "Total reward for this ep(384): 102.73\n",
      "This epsiode lasted 1023 steps\n",
      "Total reward for this ep(385): 117.30\n",
      "This epsiode lasted 1323 steps\n",
      "Total reward for this ep(386): 82.74\n",
      "This epsiode lasted 1014 steps\n",
      "Total reward for this ep(387): 107.71\n",
      "This epsiode lasted 1027 steps\n",
      "Total reward for this ep(388): 97.72\n",
      "This epsiode lasted 1273 steps\n",
      "Total reward for this ep(389): 100.24\n",
      "This epsiode lasted 1021 steps\n",
      "Total reward for this ep(390): 97.59\n",
      "This epsiode lasted 1035 steps\n",
      "Total reward for this ep(391): 77.73\n",
      "This epsiode lasted 1013 steps\n",
      "Total reward for this ep(392): 77.77\n",
      "This epsiode lasted 1009 steps\n",
      "Total reward for this ep(393): 112.36\n",
      "This epsiode lasted 1315 steps\n",
      "Total reward for this ep(394): 77.66\n",
      "This epsiode lasted 1020 steps\n",
      "Total reward for this ep(395): 92.69\n",
      "This epsiode lasted 1023 steps\n",
      "Total reward for this ep(396): 100.05\n",
      "This epsiode lasted 1040 steps\n",
      "Total reward for this ep(397): 82.61\n",
      "This epsiode lasted 1027 steps\n",
      "Total reward for this ep(398): 95.03\n",
      "This epsiode lasted 1040 steps\n",
      "Total reward for this ep(399): 91.54\n",
      "This epsiode lasted 1138 steps\n",
      "Total reward for this ep(400): 92.74\n",
      "This epsiode lasted 1018 steps\n",
      "Total reward for this ep(401): 94.37\n",
      "This epsiode lasted 1106 steps\n",
      "Total reward for this ep(402): 97.71\n",
      "This epsiode lasted 1023 steps\n",
      "Total reward for this ep(403): 100.31\n",
      "This epsiode lasted 1014 steps\n",
      "Total reward for this ep(404): 60.12\n",
      "This epsiode lasted 1017 steps\n",
      "Total reward for this ep(405): 97.69\n",
      "This epsiode lasted 1025 steps\n",
      "Total reward for this ep(406): 75.17\n",
      "This epsiode lasted 1018 steps\n",
      "Total reward for this ep(407): 82.30\n",
      "This epsiode lasted 1058 steps\n",
      "Total reward for this ep(408): 107.58\n",
      "This epsiode lasted 1040 steps\n",
      "Total reward for this ep(409): 75.32\n",
      "This epsiode lasted 1003 steps\n",
      "Total reward for this ep(410): 92.71\n",
      "This epsiode lasted 1021 steps\n",
      "Total reward for this ep(411): 85.11\n",
      "This epsiode lasted 1028 steps\n",
      "Total reward for this ep(412): 86.49\n",
      "This epsiode lasted 1141 steps\n",
      "Total reward for this ep(413): 85.19\n",
      "This epsiode lasted 1020 steps\n",
      "Total reward for this ep(414): 109.06\n",
      "This epsiode lasted 1143 steps\n",
      "Total reward for this ep(415): 77.70\n",
      "This epsiode lasted 1016 steps\n",
      "Total reward for this ep(416): 87.67\n",
      "This epsiode lasted 1023 steps\n",
      "Total reward for this ep(417): 76.92\n",
      "This epsiode lasted 1094 steps\n",
      "Total reward for this ep(418): 75.07\n",
      "This epsiode lasted 1028 steps\n",
      "Total reward for this ep(419): 107.77\n",
      "This epsiode lasted 1021 steps\n",
      "Total reward for this ep(420): 100.25\n",
      "This epsiode lasted 1020 steps\n",
      "Total reward for this ep(421): 82.73\n",
      "This epsiode lasted 1015 steps\n",
      "Total reward for this ep(422): 87.77\n",
      "This epsiode lasted 1013 steps\n",
      "Total reward for this ep(423): 82.60\n",
      "This epsiode lasted 1028 steps\n",
      "Total reward for this ep(424): 100.37\n",
      "This epsiode lasted 1008 steps\n",
      "Total reward for this ep(425): 97.67\n",
      "This epsiode lasted 1027 steps\n",
      "Total reward for this ep(426): 55.06\n",
      "This epsiode lasted 1021 steps\n",
      "Total reward for this ep(427): 120.65\n",
      "This epsiode lasted 1239 steps\n",
      "Total reward for this ep(428): 81.90\n",
      "This epsiode lasted 1098 steps\n",
      "Total reward for this ep(429): 87.69\n",
      "This epsiode lasted 1021 steps\n",
      "Total reward for this ep(430): 106.43\n",
      "This epsiode lasted 1155 steps\n",
      "Total reward for this ep(431): 90.12\n",
      "This epsiode lasted 1029 steps\n",
      "Total reward for this ep(432): 84.82\n",
      "This epsiode lasted 1057 steps\n",
      "Total reward for this ep(433): 90.15\n",
      "This epsiode lasted 1026 steps\n",
      "Total reward for this ep(434): 110.66\n",
      "This epsiode lasted 1234 steps\n",
      "Total reward for this ep(435): 77.56\n",
      "This epsiode lasted 1030 steps\n",
      "Total reward for this ep(436): 89.73\n",
      "This epsiode lasted 1068 steps\n",
      "Total reward for this ep(437): 101.93\n",
      "This epsiode lasted 1103 steps\n",
      "Total reward for this ep(438): 87.35\n",
      "This epsiode lasted 1055 steps\n",
      "Total reward for this ep(439): 92.74\n",
      "This epsiode lasted 1018 steps\n",
      "Total reward for this ep(440): 131.12\n",
      "This epsiode lasted 1447 steps\n",
      "Total reward for this ep(441): 75.20\n",
      "This epsiode lasted 1015 steps\n",
      "Total reward for this ep(442): 92.72\n",
      "This epsiode lasted 1020 steps\n",
      "Total reward for this ep(443): 95.18\n",
      "This epsiode lasted 1025 steps\n",
      "Total reward for this ep(444): 97.59\n",
      "This epsiode lasted 1035 steps\n",
      "Total reward for this ep(445): 115.36\n",
      "This epsiode lasted 1015 steps\n",
      "Total reward for this ep(446): 70.25\n",
      "This epsiode lasted 1008 steps\n",
      "Total reward for this ep(447): 90.24\n",
      "This epsiode lasted 1017 steps\n",
      "Total reward for this ep(448): 89.45\n",
      "This epsiode lasted 1096 steps\n",
      "Total reward for this ep(449): 92.82\n",
      "This epsiode lasted 1010 steps\n",
      "Total reward for this ep(450): 82.64\n",
      "This epsiode lasted 1024 steps\n",
      "Total reward for this ep(451): 117.89\n",
      "This epsiode lasted 1013 steps\n",
      "Total reward for this ep(452): 87.76\n",
      "This epsiode lasted 1014 steps\n",
      "Total reward for this ep(453): 98.37\n",
      "This epsiode lasted 1208 steps\n",
      "Total reward for this ep(454): 113.93\n",
      "This epsiode lasted 1409 steps\n",
      "Total reward for this ep(455): 90.32\n",
      "This epsiode lasted 1009 steps\n",
      "Total reward for this ep(456): 97.86\n",
      "This epsiode lasted 1008 steps\n",
      "Total reward for this ep(457): 101.46\n",
      "This epsiode lasted 1150 steps\n",
      "Total reward for this ep(458): 90.26\n",
      "This epsiode lasted 1015 steps\n",
      "Total reward for this ep(459): 119.20\n",
      "This epsiode lasted 1133 steps\n",
      "Total reward for this ep(460): 107.01\n",
      "This epsiode lasted 1097 steps\n",
      "Total reward for this ep(461): 77.66\n",
      "This epsiode lasted 1020 steps\n",
      "Total reward for this ep(462): 89.04\n",
      "This epsiode lasted 1137 steps\n",
      "Total reward for this ep(463): 100.21\n",
      "This epsiode lasted 1024 steps\n",
      "Total reward for this ep(464): 80.16\n",
      "This epsiode lasted 1021 steps\n",
      "Total reward for this ep(465): 92.73\n",
      "This epsiode lasted 1019 steps\n",
      "Total reward for this ep(466): 87.62\n",
      "This epsiode lasted 1028 steps\n",
      "Total reward for this ep(467): 85.13\n",
      "This epsiode lasted 1026 steps\n",
      "Total reward for this ep(468): 77.75\n",
      "This epsiode lasted 1011 steps\n",
      "Total reward for this ep(469): 90.21\n",
      "This epsiode lasted 1020 steps\n",
      "Total reward for this ep(470): 80.40\n",
      "This epsiode lasted 997 steps\n",
      "Total reward for this ep(471): 75.17\n",
      "This epsiode lasted 1018 steps\n",
      "Total reward for this ep(472): 90.28\n",
      "This epsiode lasted 1013 steps\n",
      "Total reward for this ep(473): 80.14\n",
      "This epsiode lasted 1023 steps\n",
      "Total reward for this ep(474): 90.18\n",
      "This epsiode lasted 1023 steps\n",
      "Total reward for this ep(475): 82.55\n",
      "This epsiode lasted 1033 steps\n",
      "Total reward for this ep(476): 99.56\n",
      "This epsiode lasted 1089 steps\n",
      "Total reward for this ep(477): 85.13\n",
      "This epsiode lasted 1026 steps\n",
      "Total reward for this ep(478): 92.71\n",
      "This epsiode lasted 1021 steps\n",
      "Total reward for this ep(479): 92.80\n",
      "This epsiode lasted 1012 steps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-02a75f2d0482>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepisode_number\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mupdate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# boring book-keeping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-186-b7fdce8fc4d4>\u001b[0m in \u001b[0;36mupdate_policy\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_episode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Scale rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "\n",
    "# hyperparameters\n",
    "learning_rate = 1e-4 * 5\n",
    "gamma = 0.99 # discount factor for reward\n",
    "h1 = 20 # number of hidden layer neurons\n",
    "h2 = 10 # number of hidden layer neurons\n",
    "\n",
    "D_in = 2 ## 1. (where we are - where we need to go), 2. (paddle center last frame - paddle center this frame)\n",
    "\n",
    "policy = Policy2(D_in, h1=H, h2=50)\n",
    "policy = policy.to(device=device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "resume = False # resume from previous checkpoint?\n",
    "LOAD_PATH=\"models/control_save_vel2_h100\"\n",
    "save_counter = 0\n",
    "total_reward = 0\n",
    "paddle_height = 15\n",
    "\n",
    "render=False\n",
    "plotting=False\n",
    "\n",
    "if resume:\n",
    "    checkpoint = torch.load(LOAD_PATH)\n",
    "    policy.load_state_dict(checkpoint['model_state_dict'])\n",
    "    episode_number = checkpoint['episode_number']\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "steps=0\n",
    "prev_x = None # used in computing the difference frame\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "start = time.time()\n",
    "prev_paddle_y = -1\n",
    "target_loc = 55\n",
    "up_down_counter = 0\n",
    "no_op_counter = 0\n",
    "while(episode_number < 5000):\n",
    "    if render: \n",
    "        env.render()\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    # preprocess the observation\n",
    "    curr_img = prepro(observation)\n",
    "    paddle_y = get_paddle_y(curr_img)\n",
    "\n",
    "    #if paddle_y != -1:\n",
    "    if paddle_y != -1 and prev_paddle_y != -1:\n",
    "        #x = np.array([target_loc - paddle_y])\n",
    "        vel = paddle_y - prev_paddle_y\n",
    "        x = np.array([target_loc - paddle_y, vel])\n",
    "    else:\n",
    "        vel = 0\n",
    "        x = np.zeros(D_in)\n",
    "\n",
    "    # forward the policy network and sample an action from the returned probability\n",
    "    #aprobs, h = policy(x)\n",
    "    action = select_action(x)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    steps += 1\n",
    "    \n",
    "    ## ~~~~~~~~~~~~~~~~~~\n",
    "    ## Reward Assignment\n",
    "    ## ~~~~~~~~~~~~~~~~~~\n",
    "    if paddle_y == -1:\n",
    "        reward = 0\n",
    "        #no_op_counter = 0\n",
    "    elif np.abs(x[0]) < (paddle_height / 2) and vel == 0:\n",
    "        #print(\"reward achieved\")\n",
    "        reward = 2.5\n",
    "        target_loc = int(np.random.random() * 100 + 20)\n",
    "        #print(target_loc)\n",
    "    else: # punish no-ops less\n",
    "        reward = -.01\n",
    "\n",
    "    policy.reward_episode.append(reward)\n",
    "    prev_paddle_y = paddle_y\n",
    "    reward_sum += reward\n",
    "    \n",
    "    if done: # an episode finished\n",
    "        print(\"Total reward for this ep({0:d}): {1:.2f}\".format(episode_number, reward_sum))\n",
    "        episode_number += 1\n",
    "        print(\"This epsiode lasted \" + str(steps) + \" steps\")\n",
    "        steps = 0\n",
    "        \n",
    "        if episode_number % batch_size == 0:\n",
    "            update_policy()\n",
    "        \n",
    "        # boring book-keeping\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        reward_sum = 0\n",
    "        if episode_number % 100 == 0:\n",
    "            PATH = 'models/control_save_vel2_h__2layer'\n",
    "            torch.save({\n",
    "                'episode_number': episode_number,\n",
    "                'model_state_dict': policy.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, PATH)\n",
    "            #pickle.dump(model, open('models/control_save_vel2_h'+ str(H) +'_' + str(save_counter) + '.p', 'wb'))\n",
    "            #save_counter +=1\n",
    "            \n",
    "        observation = env.reset() # reset env\n",
    "        prev_x = None\n",
    "        \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = int(episode_number/20/batch_size)\n",
    "\n",
    "fig, ax1 = plt.subplots(1, 1, sharey=True, figsize=[14,9]);\n",
    "rolling_mean = pd.Series(policy.reward_history).rolling(window).mean()\n",
    "std = pd.Series(policy.reward_history).rolling(window).std()\n",
    "ax1.plot(rolling_mean)\n",
    "ax1.fill_between(range(len(policy.reward_history)),rolling_mean-std, rolling_mean+std, color='orange', alpha=0.2)\n",
    "ax1.set_title('Batch Reward Moving Average ({}-episode window)'.format(window))\n",
    "ax1.set_xlabel('Episode'); ax1.set_ylabel('Average Total reward/batch ({} episodes)'.format(batch_size))\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('results_2layer_dropout_h1_20_h2_10.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.reward_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
