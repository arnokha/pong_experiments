{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heavily adapted from \n",
    "[here](https://github.com/pytorch/examples/blob/master/vae/main.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "epochs = 1000\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                    transform=transforms.ToTensor()\n",
    "                  ),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im_dim = 28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??nn.MaxUnpool2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_space_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        #self.conv1 = nn.Conv2d(1, 5, 3, stride=1)\n",
    "        #self.maxpool1= nn.MaxPool2d(kernel_size=2, return_indices=False)\n",
    "        #self.fc21 = nn.Linear(845, 20)\n",
    "        #self.fc22 = nn.Linear(845, 20)\n",
    "        #self.fc3 = nn.Linear(20, 400)\n",
    "        #self.unconv1 = nn.ConvTranspose2d(1, 3, 3, stride=1)\n",
    "        #self.fc4 = nn.Linear(1452, 784)\n",
    "        \n",
    "        self.fc1 = nn.Conv2d(1,32, kernel_size=(28,28), stride=1, padding=0)\n",
    "        self.fc21 = nn.Conv2d(32,latent_space_size, kernel_size=(1,1), stride=1, padding=0)\n",
    "        self.fc22 = nn.Conv2d(32,latent_space_size, kernel_size=(1,1), stride=1, padding=0)\n",
    "        \n",
    "        self.fc3 = nn.ConvTranspose2d(latent_space_size,118, kernel_size=(1,1),  stride=1, padding=0)\n",
    "        self.fc4 = nn.ConvTranspose2d(118,1, kernel_size=(28,28),stride=1, padding=0)\n",
    "\n",
    "    #def encode(self, x):\n",
    "    #    h1 = F.sigmoid(self.maxpool1(self.conv1(x)))\n",
    "    #    h1 = h1.view(-1, 845)\n",
    "    #    return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    #def reparameterize(self, mu, logvar):\n",
    "    #    std = torch.exp(0.5*logvar)\n",
    "    #    eps = torch.randn_like(std)\n",
    "    #    return mu + eps*std\n",
    "\n",
    "    #def decode(self, z):\n",
    "    #    h3 = F.sigmoid(self.fc3(z)).view(-1,20,20)\n",
    "    #    h3 = h3.view(-1,1,20,20)\n",
    "    #    h3 = self.unconv1(h3)\n",
    "    #    return torch.sigmoid(self.fc4(h3.view(-1,1452)))\n",
    "\n",
    "    #def forward(self, x):\n",
    "    #    mu, logvar = self.encode(x)\n",
    "    #    z = self.reparameterize(mu, logvar)\n",
    "    #    return self.decode(z), mu, logvar\n",
    "    \n",
    "    \n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5*logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return F.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar, beta=1):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + beta * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar, beta=2)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 156.649048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arno/anaconda3/lib/python3.5/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/arno/anaconda3/lib/python3.5/site-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([128, 784])) that is different to the input size (torch.Size([128, 1, 28, 28])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 147.443207\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 152.603653\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 154.884399\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 143.495789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arno/anaconda3/lib/python3.5/site-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([96, 784])) that is different to the input size (torch.Size([96, 1, 28, 28])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 151.6794\n",
      "epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arno/anaconda3/lib/python3.5/site-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([16, 784])) that is different to the input size (torch.Size([16, 1, 28, 28])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 129.7117\n",
      "after\n",
      "2\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 146.761383\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 152.824844\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 145.263184\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 152.726562\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 145.201035\n",
      "====> Epoch: 2 Average loss: 147.1863\n",
      "epoch\n",
      "====> Test set loss: 125.7795\n",
      "after\n",
      "3\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 151.643967\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 141.818085\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 149.340500\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 142.461304\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 139.137726\n",
      "====> Epoch: 3 Average loss: 144.8802\n",
      "epoch\n",
      "====> Test set loss: 124.4922\n",
      "after\n",
      "4\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 148.883179\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 140.420898\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 145.099411\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 148.300629\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 143.578033\n",
      "====> Epoch: 4 Average loss: 143.4092\n",
      "epoch\n",
      "====> Test set loss: 123.1160\n",
      "after\n",
      "5\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 138.441696\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 137.426697\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 140.841812\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 142.404205\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 147.340103\n",
      "====> Epoch: 5 Average loss: 142.0382\n",
      "epoch\n",
      "====> Test set loss: 121.7854\n",
      "after\n",
      "6\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 145.547760\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 142.316406\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 140.399200\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 141.738358\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 139.621246\n",
      "====> Epoch: 6 Average loss: 140.8234\n",
      "epoch\n",
      "====> Test set loss: 121.0852\n",
      "after\n",
      "7\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 135.357193\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 137.438004\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 145.556580\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 140.467438\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 133.592316\n",
      "====> Epoch: 7 Average loss: 140.0654\n",
      "epoch\n",
      "====> Test set loss: 120.6847\n",
      "after\n",
      "8\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 139.561157\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 139.101303\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 137.894440\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 142.918762\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 137.730698\n",
      "====> Epoch: 8 Average loss: 139.5356\n",
      "epoch\n",
      "====> Test set loss: 119.9585\n",
      "after\n",
      "9\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 135.377396\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 141.905273\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 138.004074\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 129.772446\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 142.411011\n",
      "====> Epoch: 9 Average loss: 139.0575\n",
      "epoch\n",
      "====> Test set loss: 119.3949\n",
      "after\n",
      "10\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 143.190964\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 138.027161\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 136.349319\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 141.559509\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 144.280334\n",
      "====> Epoch: 10 Average loss: 138.5762\n",
      "epoch\n",
      "====> Test set loss: 119.1256\n",
      "after\n",
      "11\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 140.149704\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 135.198151\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 131.101364\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 136.493027\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 140.329437\n",
      "====> Epoch: 11 Average loss: 138.2068\n",
      "epoch\n",
      "====> Test set loss: 118.7414\n",
      "after\n",
      "12\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 135.063553\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 137.666977\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 137.516479\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 138.093811\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 133.094635\n",
      "====> Epoch: 12 Average loss: 137.8554\n",
      "epoch\n",
      "====> Test set loss: 118.2708\n",
      "after\n",
      "13\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 143.198959\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 137.348129\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 138.479446\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 131.069138\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 140.166946\n",
      "====> Epoch: 13 Average loss: 137.6021\n",
      "epoch\n",
      "====> Test set loss: 118.2016\n",
      "after\n",
      "14\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 138.731445\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 140.665100\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 136.614014\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 134.611725\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 138.575195\n",
      "====> Epoch: 14 Average loss: 137.3204\n",
      "epoch\n",
      "====> Test set loss: 117.8989\n",
      "after\n",
      "15\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 140.371216\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 137.209808\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 136.626175\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 141.222092\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 134.032181\n",
      "====> Epoch: 15 Average loss: 137.0847\n",
      "epoch\n",
      "====> Test set loss: 117.5383\n",
      "after\n",
      "16\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 135.012177\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 135.105713\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 134.347809\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 140.439194\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 135.038193\n",
      "====> Epoch: 16 Average loss: 136.8406\n",
      "epoch\n",
      "====> Test set loss: 117.3436\n",
      "after\n",
      "17\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 138.243790\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 134.261261\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 142.329422\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 136.070023\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 138.439362\n",
      "====> Epoch: 17 Average loss: 136.7045\n",
      "epoch\n",
      "====> Test set loss: 117.2239\n",
      "after\n",
      "18\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 134.789505\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 137.349762\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 130.449036\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 136.128403\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 137.892227\n",
      "====> Epoch: 18 Average loss: 136.4922\n",
      "epoch\n",
      "====> Test set loss: 117.1040\n",
      "after\n",
      "19\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 138.281738\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 130.872681\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 139.857056\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 133.383972\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 133.540070\n",
      "====> Epoch: 19 Average loss: 136.3189\n",
      "epoch\n",
      "====> Test set loss: 116.5966\n",
      "after\n",
      "20\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 141.826843\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 137.358154\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 132.561371\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 124.519226\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 134.535767\n",
      "====> Epoch: 20 Average loss: 136.1657\n",
      "epoch\n",
      "====> Test set loss: 116.7368\n",
      "after\n",
      "21\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 134.968933\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 135.917908\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 143.585159\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 136.279922\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 142.372375\n",
      "====> Epoch: 21 Average loss: 136.0393\n",
      "epoch\n",
      "====> Test set loss: 116.2981\n",
      "after\n",
      "22\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 137.548325\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 142.515488\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 130.110199\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 135.625839\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 133.896561\n",
      "====> Epoch: 22 Average loss: 135.9416\n",
      "epoch\n",
      "====> Test set loss: 116.2053\n",
      "after\n",
      "23\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 144.162811\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 134.667816\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 137.051254\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 134.477921\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 134.427002\n",
      "====> Epoch: 23 Average loss: 135.7719\n",
      "epoch\n",
      "====> Test set loss: 116.4859\n",
      "after\n",
      "24\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 133.765717\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 141.453873\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 134.425659\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 137.866730\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 137.920441\n",
      "====> Epoch: 24 Average loss: 135.6969\n",
      "epoch\n",
      "====> Test set loss: 116.2629\n",
      "after\n",
      "25\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 132.662521\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 142.846817\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 136.882080\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 133.751328\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 130.868774\n",
      "====> Epoch: 25 Average loss: 135.5161\n",
      "epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 115.9900\n",
      "after\n",
      "26\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 126.809418\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 134.262085\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 136.633102\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 136.460587\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 136.636139\n",
      "====> Epoch: 26 Average loss: 135.3937\n",
      "epoch\n",
      "====> Test set loss: 116.1227\n",
      "after\n",
      "27\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 131.835388\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 137.764221\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 133.530304\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 134.913651\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 135.081665\n",
      "====> Epoch: 27 Average loss: 135.3669\n",
      "epoch\n",
      "====> Test set loss: 115.7214\n",
      "after\n",
      "28\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 140.581436\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 131.157562\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 133.906357\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 134.694809\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 136.866302\n",
      "====> Epoch: 28 Average loss: 135.2675\n",
      "epoch\n",
      "====> Test set loss: 115.6508\n",
      "after\n",
      "29\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 136.350800\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 137.218613\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 131.415039\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 142.024750\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 132.291962\n",
      "====> Epoch: 29 Average loss: 135.1792\n",
      "epoch\n",
      "====> Test set loss: 115.6736\n",
      "after\n",
      "30\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 132.973969\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 136.538635\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 127.974152\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 135.544006\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 129.453339\n",
      "====> Epoch: 30 Average loss: 135.1389\n",
      "epoch\n",
      "====> Test set loss: 115.5053\n",
      "after\n",
      "31\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 136.156128\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 133.190002\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 133.832108\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 138.199203\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 133.315292\n",
      "====> Epoch: 31 Average loss: 135.0002\n",
      "epoch\n",
      "====> Test set loss: 115.7975\n",
      "after\n",
      "32\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 133.888184\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 137.986343\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 134.469360\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 137.869659\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 136.632629\n",
      "====> Epoch: 32 Average loss: 134.9570\n",
      "epoch\n",
      "====> Test set loss: 115.5605\n",
      "after\n",
      "33\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 136.175232\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 133.631012\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 137.063812\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 132.526794\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 133.537964\n",
      "====> Epoch: 33 Average loss: 134.8614\n",
      "epoch\n",
      "====> Test set loss: 115.4061\n",
      "after\n",
      "34\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 129.429718\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 139.633362\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 131.506287\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 137.800064\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 136.174530\n",
      "====> Epoch: 34 Average loss: 134.7850\n",
      "epoch\n",
      "====> Test set loss: 115.2468\n",
      "after\n",
      "35\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 135.334137\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 131.612625\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 133.693192\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 131.220612\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 132.751450\n",
      "====> Epoch: 35 Average loss: 134.6803\n",
      "epoch\n",
      "====> Test set loss: 115.0159\n",
      "after\n",
      "36\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 129.766678\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 134.984787\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 132.867310\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 132.928131\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 134.646805\n",
      "====> Epoch: 36 Average loss: 134.6707\n",
      "epoch\n",
      "====> Test set loss: 115.1891\n",
      "after\n",
      "37\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 136.474518\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 137.180542\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 135.118500\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 131.173309\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 134.476562\n",
      "====> Epoch: 37 Average loss: 134.6562\n",
      "epoch\n",
      "====> Test set loss: 115.1164\n",
      "after\n",
      "38\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 134.832809\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 136.144226\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 136.478180\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 137.726196\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 141.219818\n",
      "====> Epoch: 38 Average loss: 134.5448\n",
      "epoch\n",
      "====> Test set loss: 114.8781\n",
      "after\n",
      "39\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 134.375931\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 131.932007\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 129.867462\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 135.793106\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 136.200409\n",
      "====> Epoch: 39 Average loss: 134.5250\n",
      "epoch\n",
      "====> Test set loss: 115.1227\n",
      "after\n",
      "40\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 136.197998\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 141.563049\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 137.893387\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 134.159943\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 138.118637\n",
      "====> Epoch: 40 Average loss: 134.4169\n",
      "epoch\n",
      "====> Test set loss: 115.0053\n",
      "after\n",
      "41\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 138.011505\n",
      "Train Epoch: 41 [12800/60000 (21%)]\tLoss: 133.481583\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 138.256317\n",
      "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 126.517075\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 129.324707\n",
      "====> Epoch: 41 Average loss: 134.4145\n",
      "epoch\n",
      "====> Test set loss: 114.8458\n",
      "after\n",
      "42\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 141.068542\n",
      "Train Epoch: 42 [12800/60000 (21%)]\tLoss: 132.143906\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 138.251709\n",
      "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 134.522461\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 132.776886\n",
      "====> Epoch: 42 Average loss: 134.3975\n",
      "epoch\n",
      "====> Test set loss: 114.7892\n",
      "after\n",
      "43\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 132.757004\n",
      "Train Epoch: 43 [12800/60000 (21%)]\tLoss: 134.774673\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 132.599579\n",
      "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 134.439102\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 131.972427\n",
      "====> Epoch: 43 Average loss: 134.3083\n",
      "epoch\n",
      "====> Test set loss: 114.6906\n",
      "after\n",
      "44\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 137.628754\n",
      "Train Epoch: 44 [12800/60000 (21%)]\tLoss: 133.742004\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 135.724091\n",
      "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 130.766266\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 131.046188\n",
      "====> Epoch: 44 Average loss: 134.1964\n",
      "epoch\n",
      "====> Test set loss: 114.5028\n",
      "after\n",
      "45\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 139.292542\n",
      "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 131.372833\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 139.321686\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 138.017303\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 137.425232\n",
      "====> Epoch: 45 Average loss: 134.2215\n",
      "epoch\n",
      "====> Test set loss: 114.6694\n",
      "after\n",
      "46\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 134.098267\n",
      "Train Epoch: 46 [12800/60000 (21%)]\tLoss: 130.593185\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 132.864487\n",
      "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 135.555344\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 136.219360\n",
      "====> Epoch: 46 Average loss: 134.2250\n",
      "epoch\n",
      "====> Test set loss: 114.6142\n",
      "after\n",
      "47\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 132.919098\n",
      "Train Epoch: 47 [12800/60000 (21%)]\tLoss: 136.218552\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 133.862762\n",
      "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 134.682281\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 128.943909\n",
      "====> Epoch: 47 Average loss: 134.0745\n",
      "epoch\n",
      "====> Test set loss: 114.4952\n",
      "after\n",
      "48\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 132.737213\n",
      "Train Epoch: 48 [12800/60000 (21%)]\tLoss: 136.002945\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 134.926575\n",
      "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 134.967133\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 135.858963\n",
      "====> Epoch: 48 Average loss: 134.1635\n",
      "epoch\n",
      "====> Test set loss: 114.7123\n",
      "after\n",
      "49\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 135.005280\n",
      "Train Epoch: 49 [12800/60000 (21%)]\tLoss: 130.071686\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 131.645752\n",
      "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 133.816727\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 129.242386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 49 Average loss: 134.1010\n",
      "epoch\n",
      "====> Test set loss: 114.2990\n",
      "after\n",
      "50\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 134.033478\n",
      "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 140.326965\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 130.998093\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 135.691895\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 132.287262\n",
      "====> Epoch: 50 Average loss: 134.0529\n",
      "epoch\n",
      "====> Test set loss: 114.5496\n",
      "after\n",
      "51\n",
      "Train Epoch: 51 [0/60000 (0%)]\tLoss: 138.736893\n",
      "Train Epoch: 51 [12800/60000 (21%)]\tLoss: 134.939514\n",
      "Train Epoch: 51 [25600/60000 (43%)]\tLoss: 141.042755\n",
      "Train Epoch: 51 [38400/60000 (64%)]\tLoss: 130.659500\n",
      "Train Epoch: 51 [51200/60000 (85%)]\tLoss: 138.492325\n",
      "====> Epoch: 51 Average loss: 134.0569\n",
      "epoch\n",
      "====> Test set loss: 114.5586\n",
      "after\n",
      "52\n",
      "Train Epoch: 52 [0/60000 (0%)]\tLoss: 132.640945\n",
      "Train Epoch: 52 [12800/60000 (21%)]\tLoss: 131.372894\n",
      "Train Epoch: 52 [25600/60000 (43%)]\tLoss: 132.401764\n",
      "Train Epoch: 52 [38400/60000 (64%)]\tLoss: 138.248260\n",
      "Train Epoch: 52 [51200/60000 (85%)]\tLoss: 134.245819\n",
      "====> Epoch: 52 Average loss: 133.9944\n",
      "epoch\n",
      "====> Test set loss: 114.5029\n",
      "after\n",
      "53\n",
      "Train Epoch: 53 [0/60000 (0%)]\tLoss: 134.905945\n",
      "Train Epoch: 53 [12800/60000 (21%)]\tLoss: 135.877563\n",
      "Train Epoch: 53 [25600/60000 (43%)]\tLoss: 127.618500\n",
      "Train Epoch: 53 [38400/60000 (64%)]\tLoss: 138.446762\n",
      "Train Epoch: 53 [51200/60000 (85%)]\tLoss: 132.573410\n",
      "====> Epoch: 53 Average loss: 133.9722\n",
      "epoch\n",
      "====> Test set loss: 114.3539\n",
      "after\n",
      "54\n",
      "Train Epoch: 54 [0/60000 (0%)]\tLoss: 136.802124\n",
      "Train Epoch: 54 [12800/60000 (21%)]\tLoss: 139.628860\n",
      "Train Epoch: 54 [25600/60000 (43%)]\tLoss: 133.836517\n",
      "Train Epoch: 54 [38400/60000 (64%)]\tLoss: 129.847916\n",
      "Train Epoch: 54 [51200/60000 (85%)]\tLoss: 130.845032\n",
      "====> Epoch: 54 Average loss: 133.9264\n",
      "epoch\n",
      "====> Test set loss: 114.3696\n",
      "after\n",
      "55\n",
      "Train Epoch: 55 [0/60000 (0%)]\tLoss: 125.964371\n",
      "Train Epoch: 55 [12800/60000 (21%)]\tLoss: 137.608276\n",
      "Train Epoch: 55 [25600/60000 (43%)]\tLoss: 134.809769\n",
      "Train Epoch: 55 [38400/60000 (64%)]\tLoss: 130.798279\n",
      "Train Epoch: 55 [51200/60000 (85%)]\tLoss: 133.007721\n",
      "====> Epoch: 55 Average loss: 133.8684\n",
      "epoch\n",
      "====> Test set loss: 114.2540\n",
      "after\n",
      "56\n",
      "Train Epoch: 56 [0/60000 (0%)]\tLoss: 131.294434\n",
      "Train Epoch: 56 [12800/60000 (21%)]\tLoss: 133.213699\n",
      "Train Epoch: 56 [25600/60000 (43%)]\tLoss: 132.634995\n",
      "Train Epoch: 56 [38400/60000 (64%)]\tLoss: 135.864258\n",
      "Train Epoch: 56 [51200/60000 (85%)]\tLoss: 133.643097\n",
      "====> Epoch: 56 Average loss: 133.8427\n",
      "epoch\n",
      "====> Test set loss: 114.4515\n",
      "after\n",
      "57\n",
      "Train Epoch: 57 [0/60000 (0%)]\tLoss: 138.423187\n",
      "Train Epoch: 57 [12800/60000 (21%)]\tLoss: 136.582123\n",
      "Train Epoch: 57 [25600/60000 (43%)]\tLoss: 133.697754\n",
      "Train Epoch: 57 [38400/60000 (64%)]\tLoss: 136.721466\n",
      "Train Epoch: 57 [51200/60000 (85%)]\tLoss: 139.616364\n",
      "====> Epoch: 57 Average loss: 133.8638\n",
      "epoch\n",
      "====> Test set loss: 114.2408\n",
      "after\n",
      "58\n",
      "Train Epoch: 58 [0/60000 (0%)]\tLoss: 131.077591\n",
      "Train Epoch: 58 [12800/60000 (21%)]\tLoss: 132.079529\n",
      "Train Epoch: 58 [25600/60000 (43%)]\tLoss: 135.478958\n",
      "Train Epoch: 58 [38400/60000 (64%)]\tLoss: 130.626862\n",
      "Train Epoch: 58 [51200/60000 (85%)]\tLoss: 138.077423\n",
      "====> Epoch: 58 Average loss: 133.8182\n",
      "epoch\n",
      "====> Test set loss: 114.4384\n",
      "after\n",
      "59\n",
      "Train Epoch: 59 [0/60000 (0%)]\tLoss: 140.596252\n",
      "Train Epoch: 59 [12800/60000 (21%)]\tLoss: 133.755905\n",
      "Train Epoch: 59 [25600/60000 (43%)]\tLoss: 133.451324\n",
      "Train Epoch: 59 [38400/60000 (64%)]\tLoss: 130.270782\n",
      "Train Epoch: 59 [51200/60000 (85%)]\tLoss: 137.334229\n",
      "====> Epoch: 59 Average loss: 133.7540\n",
      "epoch\n",
      "====> Test set loss: 114.0838\n",
      "after\n",
      "60\n",
      "Train Epoch: 60 [0/60000 (0%)]\tLoss: 127.309616\n",
      "Train Epoch: 60 [12800/60000 (21%)]\tLoss: 132.310638\n",
      "Train Epoch: 60 [25600/60000 (43%)]\tLoss: 134.684402\n",
      "Train Epoch: 60 [38400/60000 (64%)]\tLoss: 129.605133\n",
      "Train Epoch: 60 [51200/60000 (85%)]\tLoss: 128.302856\n",
      "====> Epoch: 60 Average loss: 133.7625\n",
      "epoch\n",
      "====> Test set loss: 114.1253\n",
      "after\n",
      "61\n",
      "Train Epoch: 61 [0/60000 (0%)]\tLoss: 135.428223\n",
      "Train Epoch: 61 [12800/60000 (21%)]\tLoss: 135.905243\n",
      "Train Epoch: 61 [25600/60000 (43%)]\tLoss: 132.664932\n",
      "Train Epoch: 61 [38400/60000 (64%)]\tLoss: 136.207214\n",
      "Train Epoch: 61 [51200/60000 (85%)]\tLoss: 136.306732\n",
      "====> Epoch: 61 Average loss: 133.7548\n",
      "epoch\n",
      "====> Test set loss: 114.2548\n",
      "after\n",
      "62\n",
      "Train Epoch: 62 [0/60000 (0%)]\tLoss: 131.693100\n",
      "Train Epoch: 62 [12800/60000 (21%)]\tLoss: 131.906174\n",
      "Train Epoch: 62 [25600/60000 (43%)]\tLoss: 126.084946\n",
      "Train Epoch: 62 [38400/60000 (64%)]\tLoss: 135.751190\n",
      "Train Epoch: 62 [51200/60000 (85%)]\tLoss: 131.032654\n",
      "====> Epoch: 62 Average loss: 133.7769\n",
      "epoch\n",
      "====> Test set loss: 114.2194\n",
      "after\n",
      "63\n",
      "Train Epoch: 63 [0/60000 (0%)]\tLoss: 133.373138\n",
      "Train Epoch: 63 [12800/60000 (21%)]\tLoss: 134.439072\n",
      "Train Epoch: 63 [25600/60000 (43%)]\tLoss: 135.021652\n",
      "Train Epoch: 63 [38400/60000 (64%)]\tLoss: 125.521233\n",
      "Train Epoch: 63 [51200/60000 (85%)]\tLoss: 134.202667\n",
      "====> Epoch: 63 Average loss: 133.6983\n",
      "epoch\n",
      "====> Test set loss: 114.0639\n",
      "after\n",
      "64\n",
      "Train Epoch: 64 [0/60000 (0%)]\tLoss: 137.014099\n",
      "Train Epoch: 64 [12800/60000 (21%)]\tLoss: 130.311371\n",
      "Train Epoch: 64 [25600/60000 (43%)]\tLoss: 132.611237\n",
      "Train Epoch: 64 [38400/60000 (64%)]\tLoss: 136.810867\n",
      "Train Epoch: 64 [51200/60000 (85%)]\tLoss: 131.743835\n",
      "====> Epoch: 64 Average loss: 133.6549\n",
      "epoch\n",
      "====> Test set loss: 113.7025\n",
      "after\n",
      "65\n",
      "Train Epoch: 65 [0/60000 (0%)]\tLoss: 139.248108\n",
      "Train Epoch: 65 [12800/60000 (21%)]\tLoss: 130.721466\n",
      "Train Epoch: 65 [25600/60000 (43%)]\tLoss: 131.231766\n",
      "Train Epoch: 65 [38400/60000 (64%)]\tLoss: 133.919449\n",
      "Train Epoch: 65 [51200/60000 (85%)]\tLoss: 131.634735\n",
      "====> Epoch: 65 Average loss: 133.6480\n",
      "epoch\n",
      "====> Test set loss: 113.7601\n",
      "after\n",
      "66\n",
      "Train Epoch: 66 [0/60000 (0%)]\tLoss: 131.050781\n",
      "Train Epoch: 66 [12800/60000 (21%)]\tLoss: 136.502014\n",
      "Train Epoch: 66 [25600/60000 (43%)]\tLoss: 136.489426\n",
      "Train Epoch: 66 [38400/60000 (64%)]\tLoss: 133.312332\n",
      "Train Epoch: 66 [51200/60000 (85%)]\tLoss: 137.831268\n",
      "====> Epoch: 66 Average loss: 133.6759\n",
      "epoch\n",
      "====> Test set loss: 113.6960\n",
      "after\n",
      "67\n",
      "Train Epoch: 67 [0/60000 (0%)]\tLoss: 131.662125\n",
      "Train Epoch: 67 [12800/60000 (21%)]\tLoss: 133.589020\n",
      "Train Epoch: 67 [25600/60000 (43%)]\tLoss: 132.200882\n",
      "Train Epoch: 67 [38400/60000 (64%)]\tLoss: 134.497375\n",
      "Train Epoch: 67 [51200/60000 (85%)]\tLoss: 138.410080\n",
      "====> Epoch: 67 Average loss: 133.6412\n",
      "epoch\n",
      "====> Test set loss: 113.7747\n",
      "after\n",
      "68\n",
      "Train Epoch: 68 [0/60000 (0%)]\tLoss: 132.572128\n",
      "Train Epoch: 68 [12800/60000 (21%)]\tLoss: 129.148376\n",
      "Train Epoch: 68 [25600/60000 (43%)]\tLoss: 129.099640\n",
      "Train Epoch: 68 [38400/60000 (64%)]\tLoss: 126.917099\n",
      "Train Epoch: 68 [51200/60000 (85%)]\tLoss: 137.038681\n",
      "====> Epoch: 68 Average loss: 133.6386\n",
      "epoch\n",
      "====> Test set loss: 113.7008\n",
      "after\n",
      "69\n",
      "Train Epoch: 69 [0/60000 (0%)]\tLoss: 131.844452\n",
      "Train Epoch: 69 [12800/60000 (21%)]\tLoss: 131.301971\n",
      "Train Epoch: 69 [25600/60000 (43%)]\tLoss: 135.652512\n",
      "Train Epoch: 69 [38400/60000 (64%)]\tLoss: 133.479843\n",
      "Train Epoch: 69 [51200/60000 (85%)]\tLoss: 133.185593\n",
      "====> Epoch: 69 Average loss: 133.6017\n",
      "epoch\n",
      "====> Test set loss: 114.0954\n",
      "after\n",
      "70\n",
      "Train Epoch: 70 [0/60000 (0%)]\tLoss: 136.458145\n",
      "Train Epoch: 70 [12800/60000 (21%)]\tLoss: 131.618317\n",
      "Train Epoch: 70 [25600/60000 (43%)]\tLoss: 131.091278\n",
      "Train Epoch: 70 [38400/60000 (64%)]\tLoss: 127.552254\n",
      "Train Epoch: 70 [51200/60000 (85%)]\tLoss: 130.129913\n",
      "====> Epoch: 70 Average loss: 133.4856\n",
      "epoch\n",
      "====> Test set loss: 113.8022\n",
      "after\n",
      "71\n",
      "Train Epoch: 71 [0/60000 (0%)]\tLoss: 131.609802\n",
      "Train Epoch: 71 [12800/60000 (21%)]\tLoss: 131.089355\n",
      "Train Epoch: 71 [25600/60000 (43%)]\tLoss: 131.272842\n",
      "Train Epoch: 71 [38400/60000 (64%)]\tLoss: 135.639099\n",
      "Train Epoch: 71 [51200/60000 (85%)]\tLoss: 127.214188\n",
      "====> Epoch: 71 Average loss: 133.5051\n",
      "epoch\n",
      "====> Test set loss: 114.1223\n",
      "after\n",
      "72\n",
      "Train Epoch: 72 [0/60000 (0%)]\tLoss: 131.341034\n",
      "Train Epoch: 72 [12800/60000 (21%)]\tLoss: 130.838516\n",
      "Train Epoch: 72 [25600/60000 (43%)]\tLoss: 133.999069\n",
      "Train Epoch: 72 [38400/60000 (64%)]\tLoss: 131.763550\n",
      "Train Epoch: 72 [51200/60000 (85%)]\tLoss: 133.563614\n",
      "====> Epoch: 72 Average loss: 133.4793\n",
      "epoch\n",
      "====> Test set loss: 113.9843\n",
      "after\n",
      "73\n",
      "Train Epoch: 73 [0/60000 (0%)]\tLoss: 132.016296\n",
      "Train Epoch: 73 [12800/60000 (21%)]\tLoss: 131.835953\n",
      "Train Epoch: 73 [25600/60000 (43%)]\tLoss: 138.856277\n",
      "Train Epoch: 73 [38400/60000 (64%)]\tLoss: 135.069580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 73 [51200/60000 (85%)]\tLoss: 128.774719\n",
      "====> Epoch: 73 Average loss: 133.4961\n",
      "epoch\n",
      "====> Test set loss: 113.9801\n",
      "after\n",
      "74\n",
      "Train Epoch: 74 [0/60000 (0%)]\tLoss: 134.792175\n",
      "Train Epoch: 74 [12800/60000 (21%)]\tLoss: 131.732330\n",
      "Train Epoch: 74 [25600/60000 (43%)]\tLoss: 135.843353\n",
      "Train Epoch: 74 [38400/60000 (64%)]\tLoss: 134.529556\n",
      "Train Epoch: 74 [51200/60000 (85%)]\tLoss: 137.299469\n",
      "====> Epoch: 74 Average loss: 133.4821\n",
      "epoch\n",
      "====> Test set loss: 113.7848\n",
      "after\n",
      "75\n",
      "Train Epoch: 75 [0/60000 (0%)]\tLoss: 135.775970\n",
      "Train Epoch: 75 [12800/60000 (21%)]\tLoss: 129.356247\n",
      "Train Epoch: 75 [25600/60000 (43%)]\tLoss: 137.485352\n",
      "Train Epoch: 75 [38400/60000 (64%)]\tLoss: 134.963455\n",
      "Train Epoch: 75 [51200/60000 (85%)]\tLoss: 131.786423\n",
      "====> Epoch: 75 Average loss: 133.4587\n",
      "epoch\n",
      "====> Test set loss: 113.8615\n",
      "after\n",
      "76\n",
      "Train Epoch: 76 [0/60000 (0%)]\tLoss: 132.610794\n",
      "Train Epoch: 76 [12800/60000 (21%)]\tLoss: 137.577591\n",
      "Train Epoch: 76 [25600/60000 (43%)]\tLoss: 139.363724\n",
      "Train Epoch: 76 [38400/60000 (64%)]\tLoss: 130.774185\n",
      "Train Epoch: 76 [51200/60000 (85%)]\tLoss: 132.153488\n",
      "====> Epoch: 76 Average loss: 133.4327\n",
      "epoch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-394ac3b238e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"after\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#if epochs % 10 == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-145-8ebbf22d8dff>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arno/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arno/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arno/anaconda3/lib/python3.5/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arno/anaconda3/lib/python3.5/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \"\"\"\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arno/anaconda3/lib/python3.5/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Main\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(epoch)\n",
    "    train(epoch)\n",
    "    print(\"epoch\")\n",
    "    test(epoch)\n",
    "    print(\"after\")\n",
    "    #if epochs % 10 == 0:\n",
    "    #    with torch.no_grad():\n",
    "    #        sample = torch.randn(64, 20).to(device)\n",
    "    #        sample = model.decode(sample).cpu()\n",
    "    #        save_image(sample.view(64, 1, 28, 28),'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1452.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8448 * 22 / 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.42857142857143"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2560 / 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
